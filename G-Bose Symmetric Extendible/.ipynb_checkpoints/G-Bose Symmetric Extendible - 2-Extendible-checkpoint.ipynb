{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442a0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "from qiskit import *\n",
    "from qiskit import IBMQ\n",
    "\n",
    "# Importing standard Qiskit libraries\n",
    "from qiskit.tools.jupyter import *\n",
    "from qiskit.visualization import *\n",
    "from qiskit.providers.aer import QasmSimulator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "from qiskit.utils import QuantumInstance\n",
    "\n",
    "from qiskit.opflow import I, X, Y, Z\n",
    "from qiskit.opflow import StateFn\n",
    "from qiskit.opflow import Gradient\n",
    "from qiskit.opflow import CircuitSampler\n",
    "from qiskit.opflow.primitive_ops import MatrixOp\n",
    "\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "\n",
    "from qiskit.algorithms.optimizers import SPSA\n",
    "from qiskit.algorithms.optimizers import GradientDescent\n",
    "from qiskit.algorithms.optimizers import ADAM\n",
    "\n",
    "from qiskit.providers.aer.noise import NoiseModel\n",
    "\n",
    "from qiskit.quantum_info import Statevector\n",
    "from qiskit.ignis.mitigation.measurement import CompleteMeasFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0f23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = IBMQ.load_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5951164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_numb(low_in,high_in):\n",
    "    '''\n",
    "    Returns a random number between low_in and high_in, including both end points.\n",
    "    '''\n",
    "    return np.random.uniform(low=low_in, high=np.nextafter(high_in, np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b39d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define variables for the number of layers and qubits of the prover. Define global variables used in the optimization.\n",
    "'''\n",
    "numLayer = 2\n",
    "numQubit = 1\n",
    "global iterations\n",
    "global noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70b9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createProver(numQubit, numLayer):\n",
    "    '''\n",
    "    Creates a parameterized unitary on numQubit qubits. Applies numLayer layers of the HEA with 2 Qiskit.parameters \n",
    "    per qubit per layer, specifying rotations about the x- and y-axes. After each layer, applies a neighbouring qubit\n",
    "    CNOT layer. Returns a QuantumCircuit object containing the parameterized prover. \n",
    "    '''\n",
    "    numparam = numQubit*numLayer*2\n",
    "    prover = QuantumCircuit(numQubit)\n",
    "    param_vector = ParameterVector(\"params\", numparam)\n",
    "    \n",
    "    for j in range(numLayer):\n",
    "        for i in range(numQubit):\n",
    "            prover.rx(param_vector[j*2*numQubit + i], i)\n",
    "            prover.ry(param_vector[j*2*numQubit + i + numQubit], i)\n",
    "\n",
    "        for i in range(numQubit-1):\n",
    "            prover.cx(i, i+1)\n",
    "            \n",
    "    return prover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80fe7e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defines parameters that specify the purification of state to be tested. Create a QuantumCircuit object that\n",
    "creates the purification on (size) number of qubits.\n",
    "'''\n",
    "unitary_param_1 = np.array([])\n",
    "size = 3\n",
    "layers = 2\n",
    "\n",
    "for i in range(0, 2*size*layers):\n",
    "    unitary_param_1 = np.append(unitary_param_1, np.array([rand_numb(0,2*np.pi)]))\n",
    "\n",
    "state = QuantumCircuit(size)\n",
    "for j in range(layers):\n",
    "    for i in range(size):\n",
    "        state.rx(unitary_param_1[j*size*2 + i], i)\n",
    "        state.ry(unitary_param_1[j*2*size + i + size], i)\n",
    "\n",
    "    for i in range(size-1):\n",
    "        state.cx(i, i+1)\n",
    "\n",
    "state.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fcd4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEqSuperpos2BE():\n",
    "    '''\n",
    "    Defines a quantum circuit to create the uniform superposition of basis elements required.\n",
    "    '''\n",
    "    temp = QuantumCircuit(1)\n",
    "    temp.h(0)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4092139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createQuantumCircuit(state, numQubit, numLayer):\n",
    "    '''\n",
    "    Creates quantum circuit for the problem. \n",
    "    Refer to Section 6.D from https://arxiv.org/pdf/2105.12758\n",
    "    The ciruit contains 5 qubits each labelled as (C, S, S')\n",
    "    '''\n",
    "    circ = QuantumCircuit(4)\n",
    "    \n",
    "    superPosCircuit = createEqSuperpos2BE()\n",
    "    circ.append(superPosCircuit, [0])\n",
    "\n",
    "    circ.append(state, [1, 2, 3])\n",
    "    \n",
    "    prover = createProver(numQubit, numLayer)\n",
    "    circ.append(prover, [3])\n",
    "    \n",
    "    circ.cswap(0, 2, 3)\n",
    "    \n",
    "    circ.append(superPosCircuit.inverse(), [0])\n",
    "    \n",
    "    return circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0893246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1207.22x367.889 with 1 Axes>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Creates the quantum circuit\n",
    "'''\n",
    "qCirc = createQuantumCircuit(qcTest, numQubit, numLayer)\n",
    "qCirc.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14898389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterView([ParameterVectorElement(params[0]), ParameterVectorElement(params[1]), ParameterVectorElement(params[2]), ParameterVectorElement(params[3])])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "List of parameters of the quantum circuit.\n",
    "'''\n",
    "print(qCirc.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ad3343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pick a backend to use. We use the statevector_simulator for noiseless simulation.\n",
    "'''\n",
    "noiseless_backend = Aer.get_backend('statevector_simulator')\n",
    "noiseless_q_instance = QuantumInstance(noiseless_backend)\n",
    "noiseless_sampler = CircuitSampler(noiseless_q_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c13d9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pick a backend to use. We use the qasm_simulator for noisy simulation.\n",
    "'''\n",
    "provider = IBMQ.get_provider(hub=\"ibm-q-research\", group=\"louisiana-st-uni-1\", project=\"main\")\n",
    "noisy_backend = Aer.get_backend(\"qasm_simulator\")\n",
    "device = provider.get_backend(\"ibmq_jakarta\")\n",
    "coupling_map = device.configuration().coupling_map\n",
    "noise_model = NoiseModel.from_backend(device.properties())\n",
    "noisy_q_instance = QuantumInstance(backend=noisy_backend, \n",
    "                           shots=8192, \n",
    "                           noise_model=noise_model, \n",
    "                           coupling_map=coupling_map,\n",
    "                           measurement_error_mitigation_cls=CompleteMeasFitter,\n",
    "                           cals_matrix_refresh_period=30)\n",
    "noisy_sampler = CircuitSampler(noisy_q_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "360335df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costf(params):\n",
    "    '''\n",
    "    Assigns the params input to the parameters of the ansatz, and calculates the expectation value.\n",
    "    '''\n",
    "    expectation = StateFn(hamiltonian, is_measurement=True).compose(StateFn(qCirc))\n",
    "    value_dict = dict(zip(qCirc.parameters, params))\n",
    "    \n",
    "    if noisy:\n",
    "        result = noisy_sampler.convert(expectation, params=value_dict).eval()  \n",
    "    else:\n",
    "        result = noiseless_sampler.convert(expectation, params=value_dict).eval()  \n",
    "    \n",
    "    return -1*np.real(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc3dce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(params):\n",
    "    '''\n",
    "    Assigns the params input to the parameters of the ansatz, and calculates the gradient value.\n",
    "    '''\n",
    "    expectation = StateFn(hamiltonian, is_measurement=True).compose(StateFn(qCirc))\n",
    "    value_dict = dict(zip(qCirc.parameters, params))\n",
    "    gradient = Gradient().convert(expectation)\n",
    "    \n",
    "    if noisy:\n",
    "        result = noisy_sampler.convert(gradient, params=value_dict).eval()  \n",
    "    else:\n",
    "        result = noiseless_sampler.convert(gradient, params=value_dict).eval()  \n",
    "    \n",
    "    return -1*np.real(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aaebd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autospsa_callback(nfev, x, fx, stepsize, accepted = False):\n",
    "    '''\n",
    "    Callback function called automatically during optimization. Appends loss value, current iteration and \n",
    "    prints every iteration with a completion percentage.\n",
    "    '''\n",
    "    if (noisy == True):\n",
    "        noisy_loss.append(-1*fx)\n",
    "    else:\n",
    "        noiseless_loss.append(-1*fx)\n",
    "    print(\"Loss Value : \", -1*fx, str(nfev/3)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03d15a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Average value of this hamiltonian is the probability of measuring 0 on the 1st qubit.\n",
    "MatrixOp(np.array([[1, 0], [0, 0])) is the projector on the 0 subspace. Qiskit orders qubits in reverse.\n",
    "'''\n",
    "hamiltonian = I^I^I^MatrixOp(np.array([[1, 0], [0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "341cf860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.56951413 1.2996747  5.10383396 5.9228751 ]\n",
      "Initial Cost :  0.5204570307490619\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Initialize prover parameters to random angles\n",
    "'''\n",
    "numParam = 2*numQubit*numLayer\n",
    "noisy = False\n",
    "paramProver = []\n",
    "for i in range(0, numParam):\n",
    "    paramProver = np.append(paramProver, np.array([rand_numb(0,2*np.pi)]))\n",
    "print(paramProver)\n",
    "\n",
    "print(\"Initial Cost : \", -1*costf(paramProver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1e67f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "True value calculated using the SDP.\n",
    "'''\n",
    "true_value = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f32685e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost :  0.5204570307490619\n",
      "Loss Value :  0.5769363417384028 1.0%\n",
      "Loss Value :  0.8289938419800214 2.0%\n",
      "Loss Value :  0.8320595561535991 3.0%\n",
      "Loss Value :  0.8320836415879839 4.0%\n",
      "Loss Value :  0.8346053145692279 5.0%\n",
      "Loss Value :  0.8685896799044137 6.0%\n",
      "Loss Value :  0.9849216631035994 7.0%\n",
      "Loss Value :  0.9784100837467988 8.0%\n",
      "Loss Value :  0.9814175335309989 9.0%\n",
      "Loss Value :  0.9940518317520127 10.0%\n",
      "Loss Value :  0.9993569605573253 11.0%\n",
      "Loss Value :  0.999355925413289 12.0%\n",
      "Loss Value :  0.9993545973866218 13.0%\n",
      "Loss Value :  0.9995549451630888 14.0%\n",
      "Loss Value :  0.9996672349731265 15.0%\n",
      "Loss Value :  0.9999551311161673 16.0%\n",
      "Loss Value :  0.9999799023286999 17.0%\n",
      "Loss Value :  0.9999557685177851 18.0%\n",
      "Loss Value :  0.999953748686927 19.0%\n",
      "Loss Value :  0.9999370488846104 20.0%\n",
      "Loss Value :  0.999923727589755 21.0%\n",
      "Loss Value :  0.9999222422544594 22.0%\n",
      "Loss Value :  0.9999673673057731 23.0%\n",
      "Loss Value :  0.9999680444602186 24.0%\n",
      "Loss Value :  0.9999987922192282 25.0%\n",
      "Loss Value :  0.9999946351254516 26.0%\n",
      "Loss Value :  0.9999981503075976 27.0%\n",
      "Loss Value :  0.9999902487385652 28.0%\n",
      "Loss Value :  0.9999915942524321 29.0%\n",
      "Loss Value :  0.9999971334476543 30.0%\n",
      "Loss Value :  0.9999843214336585 31.0%\n",
      "Loss Value :  0.9999910212029389 32.0%\n",
      "Loss Value :  0.9999970547506651 33.0%\n",
      "Loss Value :  0.9999865361708464 34.0%\n",
      "Loss Value :  0.999986477723274 35.0%\n",
      "Loss Value :  0.9999904027227984 36.0%\n",
      "Loss Value :  0.9999955993476644 37.0%\n",
      "Loss Value :  0.9999994909022346 38.0%\n",
      "Loss Value :  0.9999923072975848 39.0%\n",
      "Loss Value :  0.9999955543021862 40.0%\n",
      "Loss Value :  0.9999865525194629 41.0%\n",
      "Loss Value :  0.9999791889177104 42.0%\n",
      "Loss Value :  0.9999922006223628 43.0%\n",
      "Loss Value :  0.9999831865934323 44.0%\n",
      "Loss Value :  0.9999855803912595 45.0%\n",
      "Loss Value :  0.99997703564228 46.0%\n",
      "Loss Value :  0.9999699735926623 47.0%\n",
      "Loss Value :  0.9999751560095211 48.0%\n",
      "Loss Value :  0.9999801354493381 49.0%\n",
      "Loss Value :  0.9999838801648482 50.0%\n",
      "Loss Value :  0.9999836095030327 51.0%\n",
      "Loss Value :  0.9999835380584983 52.0%\n",
      "Loss Value :  0.9999762730635707 53.0%\n",
      "Loss Value :  0.9999760106739563 54.0%\n",
      "Loss Value :  0.9999746677679586 55.0%\n",
      "Loss Value :  0.9999740480667085 56.0%\n",
      "Loss Value :  0.9999747331469335 57.0%\n",
      "Loss Value :  0.9999791834556633 58.0%\n",
      "Loss Value :  0.9999796057028626 59.0%\n",
      "Loss Value :  0.9999734592795723 60.0%\n",
      "Loss Value :  0.9999735730648003 61.0%\n",
      "Loss Value :  0.9999685387264377 62.0%\n",
      "Loss Value :  0.9999685729618543 63.0%\n",
      "Loss Value :  0.9999687919054939 64.0%\n",
      "Loss Value :  0.9999689223179087 65.0%\n",
      "Loss Value :  0.9999651894923174 66.0%\n",
      "Loss Value :  0.999968388736178 67.0%\n",
      "Loss Value :  0.9999729697693636 68.0%\n",
      "Loss Value :  0.9999733233202085 69.0%\n",
      "Loss Value :  0.9999733086202633 70.0%\n",
      "Loss Value :  0.999975057319918 71.0%\n",
      "Loss Value :  0.9999750778413191 72.0%\n",
      "Loss Value :  0.9999750983030649 73.0%\n",
      "Loss Value :  0.9999782928907025 74.0%\n",
      "Loss Value :  0.999994557083318 75.0%\n",
      "Loss Value :  0.9999946003530981 76.0%\n",
      "Loss Value :  0.9999946371412162 77.0%\n",
      "Loss Value :  0.999996075578694 78.0%\n",
      "Loss Value :  0.9999973846456538 79.0%\n",
      "Loss Value :  0.9999942380724666 80.0%\n",
      "Loss Value :  0.9999941224310045 81.0%\n",
      "Loss Value :  0.9999902158412326 82.0%\n",
      "Loss Value :  0.9999901991087161 83.0%\n",
      "Loss Value :  0.9999901787676155 84.0%\n",
      "Loss Value :  0.9999957853562798 85.0%\n",
      "Loss Value :  0.9999982301563862 86.0%\n",
      "Loss Value :  0.9999986356046275 87.0%\n",
      "Loss Value :  0.9999980790421198 88.0%\n",
      "Loss Value :  0.9999992471159644 89.0%\n",
      "Loss Value :  0.999995831749187 90.0%\n",
      "Loss Value :  0.9999960503740022 91.0%\n",
      "Loss Value :  0.9999964812482115 92.0%\n",
      "Loss Value :  0.9999964633077146 93.0%\n",
      "Loss Value :  0.9999964538752878 94.0%\n",
      "Loss Value :  0.9999930213764271 95.0%\n",
      "Loss Value :  0.9999929897252209 96.0%\n",
      "Loss Value :  0.9999931101687508 97.0%\n",
      "Loss Value :  0.9999965230486453 98.0%\n",
      "Loss Value :  0.9999973268035026 99.0%\n",
      "Loss Value :  0.9999969380312256 100.0%\n",
      "Loss Value :  0.9999976347734931 101.0%\n",
      "Loss Value :  0.9999976298069209 102.0%\n",
      "Loss Value :  0.9999994742230173 103.0%\n",
      "Loss Value :  0.9999996529310984 104.0%\n",
      "Loss Value :  0.9999976834501858 105.0%\n",
      "Loss Value :  0.9999946115845215 106.0%\n",
      "Loss Value :  0.999991233745706 107.0%\n",
      "Loss Value :  0.9999879607130465 108.0%\n",
      "Loss Value :  0.9999880877675884 109.0%\n",
      "Loss Value :  0.9999903850883746 110.0%\n",
      "Loss Value :  0.9999920395442438 111.0%\n",
      "Loss Value :  0.9999888347238927 112.0%\n",
      "Loss Value :  0.9999896772276208 113.0%\n",
      "Loss Value :  0.999992253857409 114.0%\n",
      "Loss Value :  0.9999924537487075 115.0%\n",
      "Loss Value :  0.999989378144837 116.0%\n",
      "Loss Value :  0.9999867038169746 117.0%\n",
      "Loss Value :  0.9999841164845193 118.0%\n",
      "Loss Value :  0.9999867685107917 119.0%\n",
      "Loss Value :  0.9999963099437547 120.0%\n",
      "Loss Value :  0.9999962875086992 121.0%\n",
      "Loss Value :  0.9999963442426535 122.0%\n",
      "Loss Value :  0.9999938100537554 123.0%\n",
      "Loss Value :  0.9999977912926581 124.0%\n",
      "Loss Value :  0.9999955696048473 125.0%\n",
      "Loss Value :  0.9999928470182303 126.0%\n",
      "Loss Value :  0.9999940424127893 127.0%\n",
      "Loss Value :  0.9999913259162718 128.0%\n",
      "Loss Value :  0.9999885533652164 129.0%\n",
      "Loss Value :  0.9999859168160166 130.0%\n",
      "Loss Value :  0.9999883047807587 131.0%\n",
      "Loss Value :  0.9999859140745092 132.0%\n",
      "Loss Value :  0.9999972206330194 133.0%\n",
      "Loss Value :  0.9999947961380643 134.0%\n",
      "Loss Value :  0.9999961206009119 135.0%\n",
      "Loss Value :  0.9999938022284134 136.0%\n",
      "Loss Value :  0.9999981469075074 137.0%\n",
      "Loss Value :  0.9999969162760806 138.0%\n",
      "Loss Value :  0.9999946571894065 139.0%\n",
      "Loss Value :  0.9999924699240172 140.0%\n",
      "Loss Value :  0.9999954818461558 141.0%\n",
      "Loss Value :  0.9999930440045282 142.0%\n",
      "Loss Value :  0.9999908555705449 143.0%\n",
      "Loss Value :  0.9999979698937196 144.0%\n",
      "Loss Value :  0.9999993968875422 145.0%\n",
      "Loss Value :  0.9999994221693133 146.0%\n",
      "Loss Value :  0.999999611501349 147.0%\n",
      "Loss Value :  0.9999996918000711 148.0%\n",
      "Loss Value :  0.9999991150483126 149.0%\n",
      "Loss Value :  0.9999990901265079 150.0%\n",
      "Loss Value :  0.9999992649847729 151.0%\n",
      "Loss Value :  0.999997512782622 152.0%\n",
      "Loss Value :  0.9999974853344197 153.0%\n",
      "Loss Value :  0.9999990443340319 154.0%\n",
      "Loss Value :  0.9999999442007483 155.0%\n",
      "Loss Value :  0.9999999562442882 156.0%\n",
      "Loss Value :  0.9999999278153495 157.0%\n",
      "Loss Value :  0.9999997421465878 158.0%\n",
      "Loss Value :  0.9999996314773307 159.0%\n",
      "Loss Value :  0.9999987223932694 160.0%\n",
      "Loss Value :  0.9999985656325355 161.0%\n",
      "Loss Value :  0.9999996089164588 162.0%\n",
      "Loss Value :  0.9999997077867674 163.0%\n",
      "Loss Value :  0.99999978088054 164.0%\n",
      "Loss Value :  0.9999998355418778 165.0%\n",
      "Loss Value :  0.9999998184260714 166.0%\n",
      "Loss Value :  0.9999998651451353 167.0%\n",
      "Loss Value :  0.9999998001110708 168.0%\n",
      "Loss Value :  0.9999997138972733 169.0%\n",
      "Loss Value :  0.9999996114420665 170.0%\n",
      "Loss Value :  0.9999998432476578 171.0%\n",
      "Loss Value :  0.9999997569317519 172.0%\n",
      "Loss Value :  0.9999998899821754 173.0%\n",
      "Loss Value :  0.9999998944034523 174.0%\n",
      "Loss Value :  0.9999993858860188 175.0%\n",
      "Loss Value :  0.9999984695087933 176.0%\n",
      "Loss Value :  0.999998518997358 177.0%\n",
      "Loss Value :  0.9999985629563447 178.0%\n",
      "Loss Value :  0.9999986230947205 179.0%\n",
      "Loss Value :  0.9999986791451253 180.0%\n",
      "Loss Value :  0.9999987313038613 181.0%\n",
      "Loss Value :  0.9999987319300488 182.0%\n",
      "Loss Value :  0.9999987634725493 183.0%\n",
      "Loss Value :  0.999999626610323 184.0%\n",
      "Loss Value :  0.9999998374649427 185.0%\n",
      "Loss Value :  0.9999991200817204 186.0%\n",
      "Loss Value :  0.9999992881814544 187.0%\n",
      "Loss Value :  0.9999979198807621 188.0%\n",
      "Loss Value :  0.9999978979968916 189.0%\n",
      "Loss Value :  0.999996580171562 190.0%\n",
      "Loss Value :  0.9999978436305621 191.0%\n",
      "Loss Value :  0.9999980118878605 192.0%\n",
      "Loss Value :  0.9999982400635955 193.0%\n",
      "Loss Value :  0.9999967194891053 194.0%\n",
      "Loss Value :  0.9999952435247037 195.0%\n",
      "Loss Value :  0.9999937369321805 196.0%\n",
      "Loss Value :  0.9999959940026591 197.0%\n",
      "Loss Value :  0.9999943001492039 198.0%\n",
      "Loss Value :  0.9999944723005483 199.0%\n",
      "Loss Value :  0.9999958107700686 200.0%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Perform the noiseless optimization using the SPSA algorithm for 200 iterations.\n",
    "'''\n",
    "noisy = False\n",
    "noiseless_loss = []\n",
    "noiselessParam = np.copy(paramProver)\n",
    "print(\"Initial Cost : \", -1*costf(noiselessParam))\n",
    "autospsa = SPSA(maxiter=200, learning_rate=None, perturbation=None, callback=autospsa_callback)\n",
    "x_opt, fx_opt, nfevs = autospsa.optimize(numParam, costf, initial_point=noiselessParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a5b8f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost :  0.5210613139877722\n",
      "Loss Value :  0.5401316876744494 1.0%\n",
      "Loss Value :  0.5542500709974706 2.0%\n",
      "Loss Value :  0.8376212804970756 3.0%\n",
      "Loss Value :  0.8819750754427893 4.0%\n",
      "Loss Value :  0.9464501398862317 5.0%\n",
      "Loss Value :  0.9505284504011189 6.0%\n",
      "Loss Value :  0.9548625977345515 7.0%\n",
      "Loss Value :  0.9585072401393435 8.0%\n",
      "Loss Value :  0.9558506386482973 9.0%\n",
      "Loss Value :  0.9365870734133576 10.0%\n",
      "Loss Value :  0.9412387731727213 11.0%\n",
      "Loss Value :  0.9630774391195895 12.0%\n",
      "Loss Value :  0.9715327677438449 13.0%\n",
      "Loss Value :  0.9656738120729002 14.0%\n",
      "Loss Value :  0.9452908015076118 15.0%\n",
      "Loss Value :  0.9644860668561717 16.0%\n",
      "Loss Value :  0.9451827445554467 17.0%\n",
      "Loss Value :  0.9391300569858494 18.0%\n",
      "Loss Value :  0.9281907129548442 19.0%\n",
      "Loss Value :  0.9332477726124102 20.0%\n",
      "Loss Value :  0.9546237197402266 21.0%\n",
      "Loss Value :  0.9642125276038331 22.0%\n",
      "Loss Value :  0.9574267867186297 23.0%\n",
      "Loss Value :  0.961303227681532 24.0%\n",
      "Loss Value :  0.9580004047311115 25.0%\n",
      "Loss Value :  0.961447700292406 26.0%\n",
      "Loss Value :  0.9678250942579044 27.0%\n",
      "Loss Value :  0.9666387658503071 28.0%\n",
      "Loss Value :  0.9618962704882468 29.0%\n",
      "Loss Value :  0.9755322881909724 30.0%\n",
      "Loss Value :  0.9564759630213318 31.0%\n",
      "Loss Value :  0.9723008733837867 32.0%\n",
      "Loss Value :  0.9774231571139419 33.0%\n",
      "Loss Value :  0.9503169593161668 34.0%\n",
      "Loss Value :  0.9644170971878195 35.0%\n",
      "Loss Value :  0.9681163607402992 36.0%\n",
      "Loss Value :  0.9606347177921645 37.0%\n",
      "Loss Value :  0.9567931076244343 38.0%\n",
      "Loss Value :  0.9492318079368077 39.0%\n",
      "Loss Value :  0.9658855512260447 40.0%\n",
      "Loss Value :  0.9573439635771535 41.0%\n",
      "Loss Value :  0.9518185413052239 42.0%\n",
      "Loss Value :  0.9666784544736525 43.0%\n",
      "Loss Value :  0.9583547667741074 44.0%\n",
      "Loss Value :  0.9622462860804734 45.0%\n",
      "Loss Value :  0.9606269013370485 46.0%\n",
      "Loss Value :  0.968108772282608 47.0%\n",
      "Loss Value :  0.9488496384863553 48.0%\n",
      "Loss Value :  0.9597049975048927 49.0%\n",
      "Loss Value :  0.9705049004797959 50.0%\n",
      "Loss Value :  0.9535409363054914 51.0%\n",
      "Loss Value :  0.9601872023141589 52.0%\n",
      "Loss Value :  0.9739517479841422 53.0%\n",
      "Loss Value :  0.970199064697626 54.0%\n",
      "Loss Value :  0.9632829792111295 55.0%\n",
      "Loss Value :  0.958431649345194 56.0%\n",
      "Loss Value :  0.9666541332359204 57.0%\n",
      "Loss Value :  0.953984959608269 58.0%\n",
      "Loss Value :  0.9597065362702205 59.0%\n",
      "Loss Value :  0.9717506975282324 60.0%\n",
      "Loss Value :  0.9621583122173382 61.0%\n",
      "Loss Value :  0.9688985539065137 62.0%\n",
      "Loss Value :  0.9511592424692243 63.0%\n",
      "Loss Value :  0.9773587833705533 64.0%\n",
      "Loss Value :  0.9533326928865922 65.0%\n",
      "Loss Value :  0.9725346497690953 66.0%\n",
      "Loss Value :  0.9683082813320069 67.0%\n",
      "Loss Value :  0.9533006602569936 68.0%\n",
      "Loss Value :  0.9779778893984239 69.0%\n",
      "Loss Value :  0.9645355717243592 70.0%\n",
      "Loss Value :  0.9666337847740003 71.0%\n",
      "Loss Value :  0.9625690039298002 72.0%\n",
      "Loss Value :  0.9704175825306602 73.0%\n",
      "Loss Value :  0.9651225339004734 74.0%\n",
      "Loss Value :  0.9657086650560592 75.0%\n",
      "Loss Value :  0.9664192250643355 76.0%\n",
      "Loss Value :  0.9638172706132595 77.0%\n",
      "Loss Value :  0.9619066464534191 78.0%\n",
      "Loss Value :  0.9747174454363249 79.0%\n",
      "Loss Value :  0.9630991860754402 80.0%\n",
      "Loss Value :  0.9666910233841661 81.0%\n",
      "Loss Value :  0.9610558771393115 82.0%\n",
      "Loss Value :  0.9641588710648014 83.0%\n",
      "Loss Value :  0.9542941523671447 84.0%\n",
      "Loss Value :  0.9567193682985193 85.0%\n",
      "Loss Value :  0.9635490076702167 86.0%\n",
      "Loss Value :  0.9629473322025974 87.0%\n",
      "Loss Value :  0.9662559502252699 88.0%\n",
      "Loss Value :  0.9576627395117351 89.0%\n",
      "Loss Value :  0.9577281348781392 90.0%\n",
      "Loss Value :  0.9609014749271596 91.0%\n",
      "Loss Value :  0.9626011504792581 92.0%\n",
      "Loss Value :  0.9718889564587768 93.0%\n",
      "Loss Value :  0.9736425978637359 94.0%\n",
      "Loss Value :  0.9655054961261298 95.0%\n",
      "Loss Value :  0.9718359207894018 96.0%\n",
      "Loss Value :  0.9601256797024439 97.0%\n",
      "Loss Value :  0.9557812942261087 98.0%\n",
      "Loss Value :  0.96509000003939 99.0%\n",
      "Loss Value :  0.9725454491454887 100.0%\n",
      "Loss Value :  0.9567156005596493 101.0%\n",
      "Loss Value :  0.9592362630436694 102.0%\n",
      "Loss Value :  0.9647191284972477 103.0%\n",
      "Loss Value :  0.9673644347511485 104.0%\n",
      "Loss Value :  0.9681378416810082 105.0%\n",
      "Loss Value :  0.9564264711884873 106.0%\n",
      "Loss Value :  0.9524865441974802 107.0%\n",
      "Loss Value :  0.9623849323656113 108.0%\n",
      "Loss Value :  0.9576636831675828 109.0%\n",
      "Loss Value :  0.9652360930830839 110.0%\n",
      "Loss Value :  0.9688201919952508 111.0%\n",
      "Loss Value :  0.955288863636757 112.0%\n",
      "Loss Value :  0.9692598465529324 113.0%\n",
      "Loss Value :  0.9679677937085134 114.0%\n",
      "Loss Value :  0.9682694612001362 115.0%\n",
      "Loss Value :  0.9726435393702154 116.0%\n",
      "Loss Value :  0.9700391780597541 117.0%\n",
      "Loss Value :  0.9523289910647263 118.0%\n",
      "Loss Value :  0.9603119447762782 119.0%\n",
      "Loss Value :  0.9679266407560503 120.0%\n",
      "Loss Value :  0.9594496607413052 121.0%\n",
      "Loss Value :  0.9654978930804048 122.0%\n",
      "Loss Value :  0.9742066806766783 123.0%\n",
      "Loss Value :  0.9728470384989802 124.0%\n",
      "Loss Value :  0.9712971672208224 125.0%\n",
      "Loss Value :  0.9594922160840368 126.0%\n",
      "Loss Value :  0.9456565896806671 127.0%\n",
      "Loss Value :  0.9745554088439145 128.0%\n",
      "Loss Value :  0.9654227101531307 129.0%\n",
      "Loss Value :  0.9628635787973967 130.0%\n",
      "Loss Value :  0.9691659751823405 131.0%\n",
      "Loss Value :  0.9561297468815426 132.0%\n",
      "Loss Value :  0.9683101724862684 133.0%\n",
      "Loss Value :  0.9548931892579188 134.0%\n",
      "Loss Value :  0.9634847876539001 135.0%\n",
      "Loss Value :  0.9685592022618628 136.0%\n",
      "Loss Value :  0.9568078996169875 137.0%\n",
      "Loss Value :  0.9474043385254041 138.0%\n",
      "Loss Value :  0.9665068286069026 139.0%\n",
      "Loss Value :  0.967435230228785 140.0%\n",
      "Loss Value :  0.9603134152686407 141.0%\n",
      "Loss Value :  0.9561711466205356 142.0%\n",
      "Loss Value :  0.9482113997273537 143.0%\n",
      "Loss Value :  0.9705847525623428 144.0%\n",
      "Loss Value :  0.9701025999557968 145.0%\n",
      "Loss Value :  0.9477890850090022 146.0%\n",
      "Loss Value :  0.9717505648484679 147.0%\n",
      "Loss Value :  0.964675244212543 148.0%\n",
      "Loss Value :  0.9677781586684329 149.0%\n",
      "Loss Value :  0.9661822527449664 150.0%\n",
      "Loss Value :  0.9633442561755218 151.0%\n",
      "Loss Value :  0.9494051253999981 152.0%\n",
      "Loss Value :  0.9728070301127407 153.0%\n",
      "Loss Value :  0.969903990729364 154.0%\n",
      "Loss Value :  0.9526654737341032 155.0%\n",
      "Loss Value :  0.9675887727396408 156.0%\n",
      "Loss Value :  0.9689791579690188 157.0%\n",
      "Loss Value :  0.9511403300483091 158.0%\n",
      "Loss Value :  0.9665368492312328 159.0%\n",
      "Loss Value :  0.9711241050294213 160.0%\n",
      "Loss Value :  0.9739755849389056 161.0%\n",
      "Loss Value :  0.970024403420491 162.0%\n",
      "Loss Value :  0.9683053692806208 163.0%\n",
      "Loss Value :  0.9658989698381526 164.0%\n",
      "Loss Value :  0.9587829820246883 165.0%\n",
      "Loss Value :  0.9586254388530182 166.0%\n",
      "Loss Value :  0.9580289571306533 167.0%\n",
      "Loss Value :  0.9652902032844877 168.0%\n",
      "Loss Value :  0.9662265801732861 169.0%\n",
      "Loss Value :  0.9631066536925879 170.0%\n",
      "Loss Value :  0.9638934567363423 171.0%\n",
      "Loss Value :  0.9740175071491288 172.0%\n",
      "Loss Value :  0.9559808274721754 173.0%\n",
      "Loss Value :  0.9543992481243813 174.0%\n",
      "Loss Value :  0.9673364091512828 175.0%\n",
      "Loss Value :  0.9657801971445317 176.0%\n",
      "Loss Value :  0.9481698181274926 177.0%\n",
      "Loss Value :  0.9574603646393336 178.0%\n",
      "Loss Value :  0.9474998607071675 179.0%\n",
      "Loss Value :  0.9629088856393471 180.0%\n",
      "Loss Value :  0.9651643919206019 181.0%\n",
      "Loss Value :  0.9542447798523341 182.0%\n",
      "Loss Value :  0.9531680431869601 183.0%\n",
      "Loss Value :  0.9563884720506874 184.0%\n",
      "Loss Value :  0.9694736789792919 185.0%\n",
      "Loss Value :  0.9515974408191622 186.0%\n",
      "Loss Value :  0.9618389759274405 187.0%\n",
      "Loss Value :  0.9550362799749406 188.0%\n",
      "Loss Value :  0.9716330944853493 189.0%\n",
      "Loss Value :  0.9761142521181388 190.0%\n",
      "Loss Value :  0.9670879519235297 191.0%\n",
      "Loss Value :  0.9547261914265655 192.0%\n",
      "Loss Value :  0.9573273791548605 193.0%\n",
      "Loss Value :  0.9635205276864411 194.0%\n",
      "Loss Value :  0.9620629704398536 195.0%\n",
      "Loss Value :  0.9576486435233011 196.0%\n",
      "Loss Value :  0.9546043160925018 197.0%\n",
      "Loss Value :  0.9633759444303693 198.0%\n",
      "Loss Value :  0.9751369179396915 199.0%\n",
      "Loss Value :  0.9583956698651962 200.0%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Perform the noisy optimization using the SPSA algorithm for 200 iterations.\n",
    "'''\n",
    "noisy = True\n",
    "noisy_loss = []\n",
    "noisyParam = np.copy(paramProver)\n",
    "-1*costf(noisyParam)\n",
    "print(\"Initial Cost : \", -1*costf(noisyParam))\n",
    "autospsa = SPSA(maxiter=200, learning_rate=None, perturbation=None, callback=autospsa_callback)\n",
    "x_opt, fx_opt, nfevs = autospsa.optimize(numParam, costf, initial_point=noisyParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b530ac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995263094711919\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Evaluate the noiseless cost function using the parameters learned from the noisy optimization. \n",
    "'''\n",
    "noisy = False\n",
    "noiseResilientValue = -1*costf(x_opt)\n",
    "print(noiseResilientValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a0d66262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value :  1.0\n",
      "Noiseless :  0.9999958107700686\n",
      "Noisy :  0.9583956698651962\n",
      "Noise Resilient Value :  0.9995263094711919\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Print all final values after training.\n",
    "'''\n",
    "print(\"True Value : \", true_value)\n",
    "print(\"Noiseless : \", noiseless_loss[-1])\n",
    "print(\"Noisy : \", noisy_loss[-1])\n",
    "print(\"Noise Resilient Value : \", noiseResilientValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3bd61cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile():\n",
    "    '''\n",
    "    Write the training data to a text file to be used to plot the data. The format is as follows:\n",
    "        True Value\n",
    "        Noise Resilient Value\n",
    "        Size of noiseless data list\n",
    "        [\n",
    "        Noiseless data with one entry per line\n",
    "        ]\n",
    "        Size of noisy data list\n",
    "        [\n",
    "        Noisy data with one entry per line\n",
    "        ]\n",
    "    '''\n",
    "    file = open(\"S2_GBSE.txt\", \"w+\")\n",
    "    file.write(str(true_value)+\"\\n\")\n",
    "    file.write(str(noiseResilientValue)+\"\\n\")\n",
    "\n",
    "    file.write(str(len(noiseless_loss))+\"\\n\")\n",
    "    L = [str(i)+\"\\n\" for i in noiseless_loss]\n",
    "    file.writelines(L)\n",
    "    \n",
    "    file.write(str(len(noisy_loss))+\"\\n\")\n",
    "    L = [str(i)+\"\\n\" for i in noisy_loss[0:len(noisy_loss)-1]]\n",
    "    file.writelines(L)\n",
    "    \n",
    "    file.write(str(noisy_loss[-1]))\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "200a5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeToFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ded148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
